{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b43af799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segundo día de Datathon\n",
    "# Intento de explorar los files de manera \"correcta\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03be370d",
   "metadata": {},
   "source": [
    "# 1. Azure storage file datalake install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c90a9206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-storage-file-datalake\n",
      "  Using cached azure_storage_file_datalake-12.12.0-py3-none-any.whl (247 kB)\n",
      "Collecting azure-core<2.0.0,>=1.28.0 (from azure-storage-file-datalake)\n",
      "  Using cached azure_core-1.28.0-py3-none-any.whl (185 kB)\n",
      "Collecting azure-storage-blob<13.0.0,>=12.17.0 (from azure-storage-file-datalake)\n",
      "  Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\kenchan11\\anaconda3\\lib\\site-packages (from azure-storage-file-datalake) (4.6.3)\n",
      "Collecting isodate>=0.6.1 (from azure-storage-file-datalake)\n",
      "  Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "Requirement already satisfied: requests>=2.18.4 in c:\\users\\kenchan11\\anaconda3\\lib\\site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-file-datalake) (2.29.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\kenchan11\\anaconda3\\lib\\site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-file-datalake) (1.16.0)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in c:\\users\\kenchan11\\anaconda3\\lib\\site-packages (from azure-storage-blob<13.0.0,>=12.17.0->azure-storage-file-datalake) (39.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\kenchan11\\anaconda3\\lib\\site-packages (from cryptography>=2.1.4->azure-storage-blob<13.0.0,>=12.17.0->azure-storage-file-datalake) (1.15.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kenchan11\\anaconda3\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-file-datalake) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kenchan11\\anaconda3\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-file-datalake) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kenchan11\\anaconda3\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-file-datalake) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kenchan11\\anaconda3\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-file-datalake) (2023.5.7)\n",
      "Requirement already satisfied: pycparser in c:\\users\\kenchan11\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob<13.0.0,>=12.17.0->azure-storage-file-datalake) (2.21)\n",
      "Installing collected packages: isodate, azure-core, azure-storage-blob, azure-storage-file-datalake\n",
      "Successfully installed azure-core-1.28.0 azure-storage-blob-12.17.0 azure-storage-file-datalake-12.12.0 isodate-0.6.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install azure-storage-file-datalake --pre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2561a1be",
   "metadata": {},
   "source": [
    "# 2. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2a446dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from azure.storage.filedatalake import FileSystemClient\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c496f9",
   "metadata": {},
   "source": [
    "# 3. Azure credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4867183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key e info del Azure Account\n",
    "#Data Stored in Azure Data Lake Storage (ADLS):\n",
    "\n",
    "storage_account = \"safactoreddatathon\"\n",
    "container= \"source-files\"\n",
    "authentication_method= \"SAS Token\"\n",
    "sas_token= \"sp=rle&st=2023-07-25T18:12:36Z&se=2023-08-13T02:12:36Z&sv=2022-11-02&sr=c&sig=l2TCTwPWN8LSM922lR%2Fw78mZWQK2ErEOQDUaCJosIaw%3D\"\n",
    "sas_url= \"https://safactoreddatathon.dfs.core.windows.net/source-files?sp=rle&st=2023-07-25T18:12:36Z&se=2023-08-13T02:12:36Z&sv=2022-11-02&sr=c&sig=l2TCTwPWN8LSM922lR%2Fw78mZWQK2ErEOQDUaCJosIaw%3D\"\n",
    "url = \"https://safactoreddatathon.dfs.core.windows.net\"\n",
    "connect_str = 'DefaultEndpointsProtocol=https;AccountName=' + storage_account + ';AccountKey=' + sas_token + ';EndpointSuffix=core.windows.net'\n",
    "#- Objects:\n",
    "metadata = \"amazon_metadata\"\n",
    "data = \"amazon_reviews\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91f1161",
   "metadata": {},
   "source": [
    "# 4. Data lake service connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a4df52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "service_client = DataLakeServiceClient(account_url=\"https://safactoreddatathon.dfs.core.windows.net\"\n",
    "                                           , credential=\"sp=rle&st=2023-07-25T18:12:36Z&se=2023-08-13T02:12:36Z&sv=2022-11-02&sr=c&sig=l2TCTwPWN8LSM922lR%2Fw78mZWQK2ErEOQDUaCJosIaw%3D\")\n",
    "\n",
    "    # Obtén el sistema de archivos del cliente\n",
    "file_system_client = service_client.get_file_system_client(file_system=\"source-files\")\n",
    "\n",
    "    # Obtén el directorio del cliente\n",
    "directory_client = file_system_client.get_directory_client(\"/\")\n",
    "\n",
    "file_paths = file_system_client.get_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f56653",
   "metadata": {},
   "source": [
    "# 5. Data extraction and transformation\n",
    " * 5.1 Iterate over the paths and extract the name of the metadata and reviews files.\n",
    " * 5.2 Decompress the files.\n",
    " * 5.3 Decode the bytes.\n",
    " * 5.4 Remove the braces '{}' from the string to get individual dictionaries.\n",
    " * 5.5 Create a Pandas DataFrame.\n",
    " * 5.6 Select the relevant columns and change some datatypes to reduce file size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbab1bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.1\n",
    "reviews_json_gz = []\n",
    "metadata_list = []\n",
    "\n",
    "for path in file_paths:\n",
    "    if \".json.gz\" in path.name:\n",
    "        if \"metadata\" in path.name:\n",
    "            metadata_list.append(path.name)\n",
    "        else:       \n",
    "            reviews_json_gz.append(path.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c70cad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_json_gz_1 = reviews_json_gz[:523]\n",
    "reviews_json_gz_2 = reviews_json_gz[len(reviews_json_gz_1):len(reviews_json_gz_1)+523]\n",
    "reviews_json_gz_3 = reviews_json_gz[len(reviews_json_gz_2):len(reviews_json_gz_2)+523]\n",
    "reviews_json_gz_4 = reviews_json_gz[len(reviews_json_gz_3):len(reviews_json_gz_3)+523]\n",
    "reviews_json_gz_5 = reviews_json_gz[len(reviews_json_gz_4):len(reviews_json_gz_4)+523]\n",
    "reviews_json_gz_6 = reviews_json_gz[len(reviews_json_gz_5):len(reviews_json_gz_5)+523]\n",
    "reviews_json_gz_7 = reviews_json_gz[len(reviews_json_gz_6):len(reviews_json_gz_6)+523]\n",
    "reviews_json_gz_8 = reviews_json_gz[len(reviews_json_gz_7):len(reviews_json_gz_7)+523]\n",
    "reviews_json_gz_9 = reviews_json_gz[len(reviews_json_gz_8):len(reviews_json_gz_8)+523]\n",
    "reviews_json_gz_10 = reviews_json_gz[len(reviews_json_gz_9):len(reviews_json_gz_9)+523]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "122ec7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_json_gz_1) == len(reviews_json_gz_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1e4667",
   "metadata": {},
   "source": [
    "* **Batch 1/10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "597276fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 523/523 [24:32<00:00,  2.82s/it] \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'processed_reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m         processed_reviews_1\u001b[39m.\u001b[39mappend(df_temp)\n\u001b[0;32m     38\u001b[0m \u001b[39m# Concatenate all dataframes into a single dataframe\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[39mif\u001b[39;00m processed_reviews:\n\u001b[0;32m     40\u001b[0m     final_df_1 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(processed_reviews_1, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     41\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'processed_reviews' is not defined"
     ]
    }
   ],
   "source": [
    "# Helper function to process a single file and return the DataFrame\n",
    "def process_file(review):\n",
    "    file_client = file_system_client.get_file_client(review)\n",
    "    download = file_client.download_file()\n",
    "\n",
    "    with gzip.open(\"test.json.gz\", \"rb\") as gzip_file:\n",
    "        data = gzip_file.read()\n",
    "\n",
    "    data_str = data.decode('utf-8')\n",
    "    data_string = data_str.strip()\n",
    "\n",
    "    data_list = [eval(d) for d in data_string.split('\\n')]\n",
    "\n",
    "    df_temp = pd.DataFrame(data_list)\n",
    "    df_temp = (df_temp.drop(['style', 'image', 'reviewerName'], axis=1)\n",
    "                        .copy())\n",
    "    df_temp['overall'] = df_temp['overall'].astype('float16').astype('int8')\n",
    "    df_temp['verified'] = df_temp['verified'].astype(bool)\n",
    "    df_temp['vote'] = df_temp['vote'].fillna(\"0\").astype('int16')\n",
    "    df_temp['Datetime'] = pd.to_datetime(df_temp['unixReviewTime'], unit='s')\n",
    "    df_temp = df_temp[df_temp['Datetime'].dt.year >= 2012].drop('unixReviewTime', axis=1)\n",
    "\n",
    "    return df_temp\n",
    "\n",
    "# Main code for parallel processing\n",
    "processed_reviews_1 = []\n",
    "\n",
    "# Assuming 'reviews_json_gz' is a list of file paths obtained from Azure service\n",
    "\n",
    "num_cpu_cores = os.cpu_count()\n",
    "max_workers = int(num_cpu_cores * 0.5)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers = max_workers) as executor:\n",
    "    # Use tqdm to display progress bar\n",
    "    for df_temp in tqdm(executor.map(process_file, reviews_json_gz_1), total=len(reviews_json_gz_1)):\n",
    "        processed_reviews_1.append(df_temp)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "if processed_reviews_1:\n",
    "    final_df_1 = pd.concat(processed_reviews_1, ignore_index=True)\n",
    "else:\n",
    "    final_df_1 = pd.DataFrame()\n",
    "\n",
    "# Now 'final_df' contains all the data from the files in the 'reviews_json_gz' list\n",
    "print(final_df_1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67195064",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_1.to_parquet('df_reviews_batch_1.gzip', compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
