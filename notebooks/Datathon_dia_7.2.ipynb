{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43af799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segundo día de Datathon\n",
    "# Intento de explorar los files de manera \"correcta\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03be370d",
   "metadata": {},
   "source": [
    "# 1. Azure storage file datalake install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08838f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90a9206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install azure-storage-file-datalake --pre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2561a1be",
   "metadata": {},
   "source": [
    "# 2. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2a446dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from azure.storage.filedatalake import FileSystemClient\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime, timedelta\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c496f9",
   "metadata": {},
   "source": [
    "# 3. Azure credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4867183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key e info del Azure Account\n",
    "#Data Stored in Azure Data Lake Storage (ADLS):\n",
    "\n",
    "storage_account = \"safactoreddatathon\"\n",
    "container= \"source-files\"\n",
    "authentication_method= \"SAS Token\"\n",
    "sas_token= \"sp=rle&st=2023-07-25T18:12:36Z&se=2023-08-13T02:12:36Z&sv=2022-11-02&sr=c&sig=l2TCTwPWN8LSM922lR%2Fw78mZWQK2ErEOQDUaCJosIaw%3D\"\n",
    "sas_url= \"https://safactoreddatathon.dfs.core.windows.net/source-files?sp=rle&st=2023-07-25T18:12:36Z&se=2023-08-13T02:12:36Z&sv=2022-11-02&sr=c&sig=l2TCTwPWN8LSM922lR%2Fw78mZWQK2ErEOQDUaCJosIaw%3D\"\n",
    "url = \"https://safactoreddatathon.dfs.core.windows.net\"\n",
    "connect_str = 'DefaultEndpointsProtocol=https;AccountName=' + storage_account + ';AccountKey=' + sas_token + ';EndpointSuffix=core.windows.net'\n",
    "#- Objects:\n",
    "metadata = \"amazon_metadata\"\n",
    "data = \"amazon_reviews\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91f1161",
   "metadata": {},
   "source": [
    "# 4. Data lake service connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9a4df52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "service_client = DataLakeServiceClient(account_url=\"https://safactoreddatathon.dfs.core.windows.net\"\n",
    "                                           , credential=\"sp=rle&st=2023-07-25T18:12:36Z&se=2023-08-13T02:12:36Z&sv=2022-11-02&sr=c&sig=l2TCTwPWN8LSM922lR%2Fw78mZWQK2ErEOQDUaCJosIaw%3D\")\n",
    "\n",
    "    # Obtén el sistema de archivos del cliente\n",
    "file_system_client = service_client.get_file_system_client(file_system=\"source-files\")\n",
    "\n",
    "    # Obtén el directorio del cliente\n",
    "directory_client = file_system_client.get_directory_client(\"/\")\n",
    "\n",
    "file_paths = file_system_client.get_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f56653",
   "metadata": {},
   "source": [
    "# 5. Data extraction and transformation\n",
    " * 5.1 Iterate over the paths and extract the name of the metadata and reviews files.\n",
    " * 5.2 Decompress the files.\n",
    " * 5.3 Decode the bytes.\n",
    " * 5.4 Remove the braces '{}' from the string to get individual dictionaries.\n",
    " * 5.5 Create a Pandas DataFrame.\n",
    " * 5.6 Select the relevant columns and change some datatypes to reduce file size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbab1bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.1\n",
    "reviews_json_gz = []\n",
    "metadata_list = []\n",
    "\n",
    "for path in file_paths:\n",
    "    if \".json.gz\" in path.name:\n",
    "        if \"metadata\" in path.name:\n",
    "            metadata_list.append(path.name)\n",
    "        else:       \n",
    "            reviews_json_gz.append(path.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c70cad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_json_gz_1 = reviews_json_gz[:523]\n",
    "reviews_json_gz_2 = reviews_json_gz[len(reviews_json_gz_1)*1: len(reviews_json_gz_1)*2]\n",
    "reviews_json_gz_3 = reviews_json_gz[len(reviews_json_gz_1)*2:len(reviews_json_gz_1)*3]\n",
    "reviews_json_gz_4 = reviews_json_gz[len(reviews_json_gz_1)*3:len(reviews_json_gz_1)*4]\n",
    "reviews_json_gz_5 = reviews_json_gz[len(reviews_json_gz_1)*4:len(reviews_json_gz_1)*5]\n",
    "reviews_json_gz_6 = reviews_json_gz[len(reviews_json_gz_1)*5:len(reviews_json_gz_1)*6]\n",
    "reviews_json_gz_7 = reviews_json_gz[len(reviews_json_gz_1)*6:len(reviews_json_gz_1)*7]\n",
    "reviews_json_gz_8 = reviews_json_gz[len(reviews_json_gz_1)*7:len(reviews_json_gz_1)*8]\n",
    "reviews_json_gz_9 = reviews_json_gz[len(reviews_json_gz_1)*8:len(reviews_json_gz_1)*9]\n",
    "reviews_json_gz_10 = reviews_json_gz[len(reviews_json_gz_1)*9:len(reviews_json_gz_1)*10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e37947",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_json_gz_2[-1] == reviews_json_gz_3[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad134833",
   "metadata": {},
   "source": [
    "* Function to process files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811612f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "games = {\n",
    "    'style': [\n",
    "        '{\"Format\":\"Video Game\"}',\n",
    "        '{\"Platform\":\"PlayStation2\"}',\n",
    "        '{\"Platform\":\"GameCube\"}',\n",
    "        '{\"Edition\":\"Standard\",\"Platform\":\"PC\"}',\n",
    "        '{\"Platform\":\"PC\"}',\n",
    "        '{\"Platform\":\"PC Download\"}',\n",
    "        '{\"Platform\":\"Xbox\"}',\n",
    "        '{\"Edition\":\"Standard\",\"Platform\":\"Mac\"}',\n",
    "        '{\"Edition\":\"Game of the Year\",\"Platform\":\"PC\"}'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7e20f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to process a single file and return the DataFrame\n",
    "def process_file(review):\n",
    "    file_client = file_system_client.get_file_client(review)\n",
    "    download = file_client.download_file()\n",
    "\n",
    "    with gzip.open(\"test.json.gz\", \"rb\") as gzip_file:\n",
    "        data = gzip_file.read()\n",
    "\n",
    "    data_str = data.decode('utf-8')\n",
    "    data_string = data_str.strip()\n",
    "\n",
    "    data_list = [eval(d) for d in data_string.split('\\n')]\n",
    "\n",
    "    df_temp = pd.DataFrame(data_list)\n",
    "    df_temp = (df_temp.drop(['image', 'reviewerName'], axis=1) #'style', \n",
    "                        .copy())\n",
    "    df_temp['overall'] = df_temp['overall'].astype('float16').astype('int8')\n",
    "    df_temp['verified'] = df_temp['verified'].astype(bool)\n",
    "    df_temp['vote'] = df_temp['vote'].fillna(\"0\").astype('int16')\n",
    "    df_temp['Datetime'] = pd.to_datetime(df_temp['unixReviewTime'], unit='s')\n",
    "    df_temp = df_temp[df_temp['Datetime'].dt.year >= 2012].drop('unixReviewTime', axis=1)\n",
    "    df_temp['YearMonth'] = df_temp['Datetime'].dt.year*100 + df_temp['Datetime'].dt.month\n",
    "    df_temp = df_temp.drop('Datetime',axis=1)\n",
    "    \n",
    "\n",
    "    df_temp_1 = df_temp.groupby(['YearMonth', 'overall', 'verified']).agg(total_count=pd.NamedAgg(column='reviewerID', aggfunc='count'),\n",
    "                                                             distinct_count=pd.NamedAgg(column='reviewerID', aggfunc='nunique'),\n",
    "                                                             votes_sum=pd.NamedAgg(column='vote', aggfunc='sum')).reset_index()\n",
    "    df_temp_2 = df_temp[df_temp['style'].str.contains('Platform') | df_temp['style'].str.contains('Game')]\n",
    "\n",
    "\n",
    "    return df_temp_1, df_temp_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1e4667",
   "metadata": {},
   "source": [
    "* **Batch 1/10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597276fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code for parallel processing\n",
    "processed_reviews_1 = []\n",
    "processed_reviews_2 = []\n",
    "# Assuming 'reviews_json_gz' is a list of file paths obtained from Azure service\n",
    "\n",
    "num_cpu_cores = os.cpu_count()\n",
    "max_workers = int(num_cpu_cores * 0.5)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers = max_workers) as executor:\n",
    "    # Use tqdm to display progress bar\n",
    "    for df_temp_1, df_temp_2 in tqdm(executor.map(process_file, reviews_json_gz_1), total=len(reviews_json_gz_1)):\n",
    "        processed_reviews_1.append(df_temp_1)\n",
    "        processed_reviews_2.append(df_temp_2)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "if processed_reviews_1:\n",
    "    final_df_grouped_1 = pd.concat(processed_reviews_1, ignore_index=True)\n",
    "else:\n",
    "    final_df_grouped_1 = pd.DataFrame()\n",
    "\n",
    "if processed_reviews_2:\n",
    "    final_df_games_1 = pd.concat(processed_reviews_2, ignore_index=True)\n",
    "else:\n",
    "    final_df_games_1 = pd.DataFrame()\n",
    "\n",
    "# Now 'final_df' contains all the data from the files in the 'reviews_json_gz' list\n",
    "\n",
    "\n",
    "final_df_grouped_1.to_parquet('GroupedAndGames/final_df_grouped_1.gzip', engine='fastparquet', compression='gzip')\n",
    "final_df_games_1.iloc[:int((final_df_games_1.index.max()/2))].to_parquet('GroupedAndGames/final_df_games_1_1.gzip', engine='fastparquet', compression='gzip')\n",
    "final_df_games_1.iloc[int((final_df_games_1.index.max()/2)):].to_parquet('GroupedAndGames/final_df_games_1_2.gzip', engine='fastparquet', compression='gzip')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6af12c",
   "metadata": {},
   "source": [
    "* Batch 2/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0794d870",
   "metadata": {},
   "outputs": [],
   "source": [
    "del processed_reviews_1\n",
    "del processed_reviews_2\n",
    "del df_temp_1\n",
    "del df_temp_2\n",
    "\n",
    "# Main code for parallel processing\n",
    "processed_reviews_1 = []\n",
    "processed_reviews_2 = []\n",
    "# Assuming 'reviews_json_gz' is a list of file paths obtained from Azure service\n",
    "\n",
    "num_cpu_cores = os.cpu_count()\n",
    "max_workers = int(num_cpu_cores * 0.5)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers = max_workers) as executor:\n",
    "    # Use tqdm to display progress bar\n",
    "    for df_temp_1, df_temp_2 in tqdm(executor.map(process_file, reviews_json_gz_2), total=len(reviews_json_gz_2)):\n",
    "        processed_reviews_1.append(df_temp_1)\n",
    "        processed_reviews_2.append(df_temp_2)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "if processed_reviews_1:\n",
    "    final_df_grouped_2 = pd.concat(processed_reviews_1, ignore_index=True)\n",
    "else:\n",
    "    final_df_grouped_2 = pd.DataFrame()\n",
    "\n",
    "if processed_reviews_2:\n",
    "    final_df_games_2 = pd.concat(processed_reviews_2, ignore_index=True)\n",
    "else:\n",
    "    final_df_games_2 = pd.DataFrame()\n",
    "\n",
    "# Now 'final_df' contains all the data from the files in the 'reviews_json_gz' list\n",
    "\n",
    "\n",
    "final_df_grouped_2.to_parquet('GroupedAndGames/final_df_grouped_2.gzip', engine='fastparquet', compression='gzip')\n",
    "final_df_games_2.iloc[:int((final_df_games_2.index.max()/2))].to_parquet('GroupedAndGames/final_df_games_2_1.gzip', engine='fastparquet', compression='gzip')\n",
    "final_df_games_2.iloc[int((final_df_games_2.index.max()/2)):].to_parquet('GroupedAndGames/final_df_games_2_2.gzip', engine='fastparquet', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabddae0",
   "metadata": {},
   "source": [
    "* Batch 3/10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b348545",
   "metadata": {},
   "outputs": [],
   "source": [
    "del processed_reviews_1\n",
    "del processed_reviews_2\n",
    "del df_temp_1\n",
    "del df_temp_2\n",
    "del final_df_grouped_2\n",
    "del final_df_games_2\n",
    "# Main code for parallel processing\n",
    "processed_reviews_1 = []\n",
    "processed_reviews_2 = []\n",
    "# Assuming 'reviews_json_gz' is a list of file paths obtained from Azure service\n",
    "\n",
    "num_cpu_cores = os.cpu_count()\n",
    "max_workers = int(num_cpu_cores * 0.5)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers = max_workers) as executor:\n",
    "    # Use tqdm to display progress bar\n",
    "    for df_temp_1, df_temp_2 in tqdm(executor.map(process_file, reviews_json_gz_3), total=len(reviews_json_gz_3)):\n",
    "        processed_reviews_1.append(df_temp_1)\n",
    "        processed_reviews_2.append(df_temp_2)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "if processed_reviews_1:\n",
    "    final_df_grouped_3 = pd.concat(processed_reviews_1, ignore_index=True)\n",
    "else:\n",
    "    final_df_grouped_3 = pd.DataFrame()\n",
    "\n",
    "if processed_reviews_2:\n",
    "    final_df_games_3 = pd.concat(processed_reviews_2, ignore_index=True)\n",
    "else:\n",
    "    final_df_games_3 = pd.DataFrame()\n",
    "\n",
    "# Now 'final_df' contains all the data from the files in the 'reviews_json_gz' list\n",
    "\n",
    "\n",
    "final_df_grouped_3.to_parquet('GroupedAndGames/grouped/final_df_grouped_3.gzip', engine='fastparquet', compression='gzip')\n",
    "final_df_games_3.iloc[:int((final_df_games_3.index.max()/2))].to_parquet('GroupedAndGames/games/final_df_games_3_1.gzip', engine='fastparquet', compression='gzip')\n",
    "final_df_games_3.iloc[int((final_df_games_3.index.max()/2)):].to_parquet('GroupedAndGames/games/final_df_games_3_2.gzip', engine='fastparquet', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed5c141",
   "metadata": {},
   "source": [
    "* Batch 4/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b28cbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del processed_reviews_1\n",
    "# del processed_reviews_2\n",
    "# del df_temp_1\n",
    "# del df_temp_2\n",
    "# del final_df_grouped_3\n",
    "# del final_df_games_3\n",
    "# Main code for parallel processing\n",
    "processed_reviews_1 = []\n",
    "processed_reviews_2 = []\n",
    "# Assuming 'reviews_json_gz' is a list of file paths obtained from Azure service\n",
    "\n",
    "num_cpu_cores = os.cpu_count()\n",
    "max_workers = int(num_cpu_cores * 0.5)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers = max_workers) as executor:\n",
    "    # Use tqdm to display progress bar\n",
    "    for df_temp_1, df_temp_2 in tqdm(executor.map(process_file, reviews_json_gz_4), total=len(reviews_json_gz_4)):\n",
    "        processed_reviews_1.append(df_temp_1)\n",
    "        processed_reviews_2.append(df_temp_2)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "if processed_reviews_1:\n",
    "    final_df_grouped_4 = pd.concat(processed_reviews_1, ignore_index=True)\n",
    "else:\n",
    "    final_df_grouped_4 = pd.DataFrame()\n",
    "\n",
    "if processed_reviews_2:\n",
    "    final_df_games_4 = pd.concat(processed_reviews_2, ignore_index=True)\n",
    "else:\n",
    "    final_df_games_4 = pd.DataFrame()\n",
    "\n",
    "# Now 'final_df' contains all the data from the files in the 'reviews_json_gz' list\n",
    "\n",
    "\n",
    "final_df_grouped_4.to_parquet('GroupedAndGames/final_df_grouped_4.gzip', engine='fastparquet', compression='gzip')\n",
    "final_df_games_4.iloc[:int((final_df_games_4.index.max()/2))].to_parquet('GroupedAndGames/final_df_games_4_1.gzip', engine='fastparquet', compression='gzip')\n",
    "final_df_games_4.iloc[int((final_df_games_4.index.max()/2)):].to_parquet('GroupedAndGames/final_df_games_4_2.gzip', engine='fastparquet', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9cca86",
   "metadata": {},
   "source": [
    "* Batch 5/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf98d130",
   "metadata": {},
   "outputs": [],
   "source": [
    "del processed_reviews_1\n",
    "del processed_reviews_2\n",
    "del df_temp_1\n",
    "del df_temp_2\n",
    "del final_df_grouped_4\n",
    "del final_df_games_4\n",
    "# Main code for parallel processing\n",
    "processed_reviews_1 = []\n",
    "processed_reviews_2 = []\n",
    "# Assuming 'reviews_json_gz' is a list of file paths obtained from Azure service\n",
    "\n",
    "num_cpu_cores = os.cpu_count()\n",
    "max_workers = int(num_cpu_cores * 0.5)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers = max_workers) as executor:\n",
    "    # Use tqdm to display progress bar\n",
    "    for df_temp_1, df_temp_2 in tqdm(executor.map(process_file, reviews_json_gz_5), total=len(reviews_json_gz_5)): # Cambiar número\n",
    "        processed_reviews_1.append(df_temp_1)\n",
    "        processed_reviews_2.append(df_temp_2)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "if processed_reviews_1:\n",
    "    final_df_grouped_5 = pd.concat(processed_reviews_1, ignore_index=True) # Cambiar número\n",
    "else:\n",
    "    final_df_grouped_5 = pd.DataFrame() # Cambiar número\n",
    "\n",
    "if processed_reviews_2:\n",
    "    final_df_games_5 = pd.concat(processed_reviews_2, ignore_index=True) # Cambiar número\n",
    "else:\n",
    "    final_df_games_5 = pd.DataFrame() # Cambiar número\n",
    "\n",
    "# Now 'final_df' contains all the data from the files in the 'reviews_json_gz' list\n",
    "\n",
    "\n",
    "final_df_grouped_5.to_parquet('GroupedAndGames/final_df_grouped_5.gzip', engine='fastparquet', compression='gzip') # Cambiar número\n",
    "final_df_games_5.iloc[:int((final_df_games_5.index.max()/2))].to_parquet('GroupedAndGames/final_df_games_5_1.gzip', engine='fastparquet', compression='gzip') # Cambiar número\n",
    "final_df_games_5.iloc[int((final_df_games_5.index.max()/2)):].to_parquet('GroupedAndGames/final_df_games_5_2.gzip', engine='fastparquet', compression='gzip') # Cambiar número"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d6f097",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_games_5[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2538a46a",
   "metadata": {},
   "source": [
    "* Batch 6/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90946c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del processed_reviews_1\n",
    "del processed_reviews_2\n",
    "del df_temp_1\n",
    "del df_temp_2\n",
    "del final_df_grouped_5\n",
    "del final_df_games_5\n",
    "# Main code for parallel processing\n",
    "processed_reviews_1 = []\n",
    "processed_reviews_2 = []\n",
    "# Assuming 'reviews_json_gz' is a list of file paths obtained from Azure service\n",
    "\n",
    "num_cpu_cores = os.cpu_count()\n",
    "max_workers = int(num_cpu_cores * 0.5)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers = max_workers) as executor:\n",
    "    # Use tqdm to display progress bar\n",
    "    for df_temp_1, df_temp_2 in tqdm(executor.map(process_file, reviews_json_gz_6), total=len(reviews_json_gz_6)): # Cambiar número\n",
    "        processed_reviews_1.append(df_temp_1)\n",
    "        processed_reviews_2.append(df_temp_2)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "if processed_reviews_1:\n",
    "    final_df_grouped_6 = pd.concat(processed_reviews_1, ignore_index=True) # Cambiar número\n",
    "else:\n",
    "    final_df_grouped_6 = pd.DataFrame() # Cambiar número\n",
    "\n",
    "if processed_reviews_2:\n",
    "    final_df_games_6 = pd.concat(processed_reviews_2, ignore_index=True) # Cambiar número\n",
    "else:\n",
    "    final_df_games_6 = pd.DataFrame() # Cambiar número\n",
    "\n",
    "# Now 'final_df' contains all the data from the files in the 'reviews_json_gz' list\n",
    "\n",
    "\n",
    "final_df_grouped_6.to_parquet('GroupedAndGames/final_df_grouped_6.gzip', engine='fastparquet', compression='gzip') # Cambiar número\n",
    "final_df_games_6.iloc[:int((final_df_games_6.index.max()/2))].to_parquet('GroupedAndGames/final_df_games_6_1.gzip', engine='fastparquet', compression='gzip') # Cambiar número\n",
    "final_df_games_6.iloc[int((final_df_games_6.index.max()/2)):].to_parquet('GroupedAndGames/final_df_games_6_2.gzip', engine='fastparquet', compression='gzip') # Cambiar número"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4412fff1",
   "metadata": {},
   "source": [
    "* Batch 7/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e1fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del processed_reviews_1\n",
    "del processed_reviews_2\n",
    "del df_temp_1\n",
    "del df_temp_2\n",
    "del final_df_grouped_6\n",
    "del final_df_games_6\n",
    "# Main code for parallel processing\n",
    "processed_reviews_1 = []\n",
    "processed_reviews_2 = []\n",
    "# Assuming 'reviews_json_gz' is a list of file paths obtained from Azure service\n",
    "\n",
    "num_cpu_cores = os.cpu_count()\n",
    "max_workers = int(num_cpu_cores * 0.5)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers = max_workers) as executor:\n",
    "    # Use tqdm to display progress bar\n",
    "    for df_temp_1, df_temp_2 in tqdm(executor.map(process_file, reviews_json_gz_7), total=len(reviews_json_gz_7)): # Cambiar número\n",
    "        processed_reviews_1.append(df_temp_1)\n",
    "        processed_reviews_2.append(df_temp_2)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "if processed_reviews_1:\n",
    "    final_df_grouped_7 = pd.concat(processed_reviews_1, ignore_index=True) # Cambiar número\n",
    "else:\n",
    "    final_df_grouped_7 = pd.DataFrame() # Cambiar número\n",
    "\n",
    "if processed_reviews_2:\n",
    "    final_df_games_7 = pd.concat(processed_reviews_2, ignore_index=True) # Cambiar número\n",
    "else:\n",
    "    final_df_games_7 = pd.DataFrame() # Cambiar número\n",
    "\n",
    "# Now 'final_df' contains all the data from the files in the 'reviews_json_gz' list\n",
    "\n",
    "\n",
    "final_df_grouped_7.to_parquet('GroupedAndGames/final_df_grouped_7.gzip', engine='fastparquet', compression='gzip') # Cambiar número\n",
    "final_df_games_7.iloc[:int((final_df_games_7.index.max()/2))].to_parquet('GroupedAndGames/final_df_games_7_1.gzip', engine='fastparquet', compression='gzip') # Cambiar número\n",
    "final_df_games_7.iloc[int((final_df_games_7.index.max()/2)):].to_parquet('GroupedAndGames/final_df_games_7_2.gzip', engine='fastparquet', compression='gzip') # Cambiar número"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64810abf",
   "metadata": {},
   "source": [
    "* Batch 8/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06a2bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del processed_reviews_1\n",
    "del processed_reviews_2\n",
    "del df_temp_1\n",
    "del df_temp_2\n",
    "del final_df_grouped_7\n",
    "del final_df_games_7\n",
    "# Main code for parallel processing\n",
    "processed_reviews_1 = []\n",
    "processed_reviews_2 = []\n",
    "# Assuming 'reviews_json_gz' is a list of file paths obtained from Azure service\n",
    "\n",
    "num_cpu_cores = os.cpu_count()\n",
    "max_workers = int(num_cpu_cores * 0.5)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers = max_workers) as executor:\n",
    "    # Use tqdm to display progress bar\n",
    "    for df_temp_1, df_temp_2 in tqdm(executor.map(process_file, reviews_json_gz_8), total=len(reviews_json_gz_8)): # Cambiar número\n",
    "        processed_reviews_1.append(df_temp_1)\n",
    "        processed_reviews_2.append(df_temp_2)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "if processed_reviews_1:\n",
    "    final_df_grouped_8 = pd.concat(processed_reviews_1, ignore_index=True) # Cambiar número\n",
    "else:\n",
    "    final_df_grouped_8 = pd.DataFrame() # Cambiar número\n",
    "\n",
    "if processed_reviews_2:\n",
    "    final_df_games_8 = pd.concat(processed_reviews_2, ignore_index=True) # Cambiar número\n",
    "else:\n",
    "    final_df_games_8 = pd.DataFrame() # Cambiar número\n",
    "\n",
    "# Now 'final_df' contains all the data from the files in the 'reviews_json_gz' list\n",
    "\n",
    "\n",
    "final_df_grouped_8.to_parquet('GroupedAndGames/final_df_grouped_8.gzip', engine='fastparquet', compression='gzip') # Cambiar número\n",
    "final_df_games_8.iloc[:int((final_df_games_8.index.max()/2))].to_parquet('GroupedAndGames/final_df_games_8_1.gzip', engine='fastparquet', compression='gzip') # Cambiar número\n",
    "final_df_games_8.iloc[int((final_df_games_8.index.max()/2)):].to_parquet('GroupedAndGames/final_df_games_8_2.gzip', engine='fastparquet', compression='gzip') # Cambiar número"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034d08c9",
   "metadata": {},
   "source": [
    "* Batch 9/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f58d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "del processed_reviews_1\n",
    "del processed_reviews_2\n",
    "del df_temp_1\n",
    "del df_temp_2\n",
    "del final_df_grouped_8\n",
    "del final_df_games_8\n",
    "# Main code for parallel processing\n",
    "processed_reviews_1 = []\n",
    "processed_reviews_2 = []\n",
    "# Assuming 'reviews_json_gz' is a list of file paths obtained from Azure service\n",
    "\n",
    "num_cpu_cores = os.cpu_count()\n",
    "max_workers = int(num_cpu_cores * 0.5)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers = max_workers) as executor:\n",
    "    # Use tqdm to display progress bar\n",
    "    for df_temp_1, df_temp_2 in tqdm(executor.map(process_file, reviews_json_gz_9), total=len(reviews_json_gz_9)): # Cambiar número\n",
    "        processed_reviews_1.append(df_temp_1)\n",
    "        processed_reviews_2.append(df_temp_2)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "if processed_reviews_1:\n",
    "    final_df_grouped_9 = pd.concat(processed_reviews_1, ignore_index=True) # Cambiar número\n",
    "else:\n",
    "    final_df_grouped_9 = pd.DataFrame() # Cambiar número\n",
    "\n",
    "if processed_reviews_2:\n",
    "    final_df_games_9 = pd.concat(processed_reviews_2, ignore_index=True) # Cambiar número\n",
    "else:\n",
    "    final_df_games_9 = pd.DataFrame() # Cambiar número\n",
    "\n",
    "# Now 'final_df' contains all the data from the files in the 'reviews_json_gz' list\n",
    "\n",
    "\n",
    "final_df_grouped_9.to_parquet('GroupedAndGames/final_df_grouped_9.gzip', engine='fastparquet', compression='gzip') # Cambiar número\n",
    "final_df_games_9.iloc[:int((final_df_games_9.index.max()/2))].to_parquet('GroupedAndGames/final_df_games_9_1.gzip', engine='fastparquet', compression='gzip') # Cambiar número\n",
    "final_df_games_9.iloc[int((final_df_games_9.index.max()/2)):].to_parquet('GroupedAndGames/final_df_games_9_2.gzip', engine='fastparquet', compression='gzip') # Cambiar número"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45299114",
   "metadata": {},
   "source": [
    "* Batch 10/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71482f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "del processed_reviews_1\n",
    "del processed_reviews_2\n",
    "del df_temp_1\n",
    "del df_temp_2\n",
    "del final_df_grouped_9\n",
    "del final_df_games_9\n",
    "# Main code for parallel processing\n",
    "processed_reviews_1 = []\n",
    "processed_reviews_2 = []\n",
    "# Assuming 'reviews_json_gz' is a list of file paths obtained from Azure service\n",
    "\n",
    "num_cpu_cores = os.cpu_count()\n",
    "max_workers = int(num_cpu_cores * 0.5)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers = max_workers) as executor:\n",
    "    # Use tqdm to display progress bar\n",
    "    for df_temp_1, df_temp_2 in tqdm(executor.map(process_file, reviews_json_gz_10), total=len(reviews_json_gz_10)): # Cambiar número\n",
    "        processed_reviews_1.append(df_temp_1)\n",
    "        processed_reviews_2.append(df_temp_2)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "if processed_reviews_1:\n",
    "    final_df_grouped_10 = pd.concat(processed_reviews_1, ignore_index=True) # Cambiar número\n",
    "else:\n",
    "    final_df_grouped_10 = pd.DataFrame() # Cambiar número\n",
    "\n",
    "if processed_reviews_2:\n",
    "    final_df_games_10 = pd.concat(processed_reviews_2, ignore_index=True) # Cambiar número\n",
    "else:\n",
    "    final_df_games_10 = pd.DataFrame() # Cambiar número\n",
    "\n",
    "# Now 'final_df' contains all the data from the files in the 'reviews_json_gz' list\n",
    "\n",
    "\n",
    "final_df_grouped_10.to_parquet('GroupedAndGames/final_df_grouped_10.gzip', engine='fastparquet', compression='gzip') # Cambiar número\n",
    "final_df_games_10.iloc[:int((final_df_games_10.index.max()/2))].to_parquet('GroupedAndGames/final_df_games_10_1.gzip', engine='fastparquet', compression='gzip') # Cambiar número\n",
    "final_df_games_10.iloc[int((final_df_games_10.index.max()/2)):].to_parquet('GroupedAndGames/final_df_games_10_2.gzip', engine='fastparquet', compression='gzip') # Cambiar número"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
