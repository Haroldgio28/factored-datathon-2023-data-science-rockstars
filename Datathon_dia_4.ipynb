{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b43af799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segundo día de Datathon\n",
    "# Intento de explorar los files de manera \"correcta\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03be370d",
   "metadata": {},
   "source": [
    "# 1. Azure storage file datalake install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c90a9206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-storage-file-datalake\n",
      "  Using cached azure_storage_file_datalake-12.12.0-py3-none-any.whl (247 kB)\n",
      "Collecting azure-core<2.0.0,>=1.28.0 (from azure-storage-file-datalake)\n",
      "  Using cached azure_core-1.28.0-py3-none-any.whl (185 kB)\n",
      "Collecting azure-storage-blob<13.0.0,>=12.17.0 (from azure-storage-file-datalake)\n",
      "  Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\kenchan11\\anaconda3\\lib\\site-packages (from azure-storage-file-datalake) (4.6.3)\n",
      "Collecting isodate>=0.6.1 (from azure-storage-file-datalake)\n",
      "  Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "Requirement already satisfied: requests>=2.18.4 in c:\\users\\kenchan11\\anaconda3\\lib\\site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-file-datalake) (2.29.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\kenchan11\\anaconda3\\lib\\site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-file-datalake) (1.16.0)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in c:\\users\\kenchan11\\anaconda3\\lib\\site-packages (from azure-storage-blob<13.0.0,>=12.17.0->azure-storage-file-datalake) (39.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\kenchan11\\anaconda3\\lib\\site-packages (from cryptography>=2.1.4->azure-storage-blob<13.0.0,>=12.17.0->azure-storage-file-datalake) (1.15.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kenchan11\\anaconda3\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-file-datalake) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kenchan11\\anaconda3\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-file-datalake) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kenchan11\\anaconda3\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-file-datalake) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kenchan11\\anaconda3\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-file-datalake) (2023.5.7)\n",
      "Requirement already satisfied: pycparser in c:\\users\\kenchan11\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob<13.0.0,>=12.17.0->azure-storage-file-datalake) (2.21)\n",
      "Installing collected packages: isodate, azure-core, azure-storage-blob, azure-storage-file-datalake\n",
      "Successfully installed azure-core-1.28.0 azure-storage-blob-12.17.0 azure-storage-file-datalake-12.12.0 isodate-0.6.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install azure-storage-file-datalake --pre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2561a1be",
   "metadata": {},
   "source": [
    "# 2. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2a446dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from azure.storage.filedatalake import FileSystemClient\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c496f9",
   "metadata": {},
   "source": [
    "# 3. Azure credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4867183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key e info del Azure Account\n",
    "#Data Stored in Azure Data Lake Storage (ADLS):\n",
    "\n",
    "storage_account = \"safactoreddatathon\"\n",
    "container= \"source-files\"\n",
    "authentication_method= \"SAS Token\"\n",
    "sas_token= \"sp=rle&st=2023-07-25T18:12:36Z&se=2023-08-13T02:12:36Z&sv=2022-11-02&sr=c&sig=l2TCTwPWN8LSM922lR%2Fw78mZWQK2ErEOQDUaCJosIaw%3D\"\n",
    "sas_url= \"https://safactoreddatathon.dfs.core.windows.net/source-files?sp=rle&st=2023-07-25T18:12:36Z&se=2023-08-13T02:12:36Z&sv=2022-11-02&sr=c&sig=l2TCTwPWN8LSM922lR%2Fw78mZWQK2ErEOQDUaCJosIaw%3D\"\n",
    "url = \"https://safactoreddatathon.dfs.core.windows.net\"\n",
    "connect_str = 'DefaultEndpointsProtocol=https;AccountName=' + storage_account + ';AccountKey=' + sas_token + ';EndpointSuffix=core.windows.net'\n",
    "#- Objects:\n",
    "metadata = \"amazon_metadata\"\n",
    "data = \"amazon_reviews\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91f1161",
   "metadata": {},
   "source": [
    "# 4. Data lake service connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a4df52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "service_client = DataLakeServiceClient(account_url=\"https://safactoreddatathon.dfs.core.windows.net\"\n",
    "                                           , credential=\"sp=rle&st=2023-07-25T18:12:36Z&se=2023-08-13T02:12:36Z&sv=2022-11-02&sr=c&sig=l2TCTwPWN8LSM922lR%2Fw78mZWQK2ErEOQDUaCJosIaw%3D\")\n",
    "\n",
    "    # Obtén el sistema de archivos del cliente\n",
    "file_system_client = service_client.get_file_system_client(file_system=\"source-files\")\n",
    "\n",
    "    # Obtén el directorio del cliente\n",
    "directory_client = file_system_client.get_directory_client(\"/\")\n",
    "\n",
    "file_paths = file_system_client.get_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f56653",
   "metadata": {},
   "source": [
    "# 5. Data extraction and transformation\n",
    " * 5.1 Iterate over the paths and extract the name of the metadata and reviews files.\n",
    " * 5.2 Decompress the files.\n",
    " * 5.3 Decode the bytes.\n",
    " * 5.4 Remove the braces '{}' from the string to get individual dictionaries.\n",
    " * 5.5 Create a Pandas DataFrame.\n",
    " * 5.6 Select the relevant columns and change some datatypes to reduce file size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbab1bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.1\n",
    "reviews_json_gz = []\n",
    "metadata_list = []\n",
    "\n",
    "for path in file_paths:\n",
    "    if \".json.gz\" in path_name:\n",
    "        if \"metadata\" in path.name:\n",
    "            metadata_list.append(path.name)\n",
    "        else:       \n",
    "            reviews_json_gz.append(path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d3eff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = reviews_list[5]\n",
    "file_client = file_system_client.get_file_client(review)\n",
    "download = file_client.download_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fa122f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(\"test.json.gz\", \"rb\") as gzip_file:\n",
    "    data = gzip_file.read()\n",
    "\n",
    "data_str = data.decode('utf-8')\n",
    "\n",
    "\n",
    "data_string = data_str.strip()\n",
    "\n",
    "data_list = [eval(d) for d in data_string.split('\\n')]\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf6c3912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55933 entries, 0 to 55932\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   asin            55933 non-null  object\n",
      " 1   overall         55933 non-null  int8  \n",
      " 2   reviewText      55912 non-null  object\n",
      " 3   reviewerID      55933 non-null  object\n",
      " 4   summary         55925 non-null  object\n",
      " 5   unixReviewTime  55933 non-null  object\n",
      " 6   verified        55933 non-null  bool  \n",
      " 7   vote            55933 non-null  int16 \n",
      "dtypes: bool(1), int16(1), int8(1), object(5)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df = (df.drop(['style', 'image', 'reviewerName'], axis=1)\n",
    "          .copy()\n",
    "          )\n",
    "df['overall'] = df['overall'].astype('float16').astype('int8')\n",
    "df['verified'] = df['verified'].astype(bool)\n",
    "df['vote'] = df['vote'].fillna(\"0\").astype('int16')\n",
    "df['Date'] = pd.to_datetime(df['unixReviewTime'], unit='s')\n",
    "df = df[df['Datetime'].dt.year >= 2012].drop('unixReviewTime', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04ca92fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('Amazon_Reviews_First_try', index=None)\n",
    "              #, partition_cols=['Year', 'overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da46131",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mdel\u001b[39;00m df\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "del df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
